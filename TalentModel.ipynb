{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec83fe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import joblib\n",
    "import pickle\n",
    "import fnmatch\n",
    "import os\n",
    "from sklearn import metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "725f579c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Phase0 (Cleaning and splting the dataset):\n",
    "\n",
    "def clean_split_data(data_copy):\n",
    "    ##############Imputing Null Values############\n",
    "    data_copy['occupation_skill_1_count']=data_copy['occupation_skill_1_count'].fillna(data_copy['occupation_skill_1_count'].mode()[0])\n",
    "    data_copy['occupation_skill_2_count']=data_copy['occupation_skill_2_count'].fillna(data_copy['occupation_skill_2_count'].mode()[0])\n",
    "    data_copy['occupation_skill_3_count']=data_copy['occupation_skill_3_count'].fillna(data_copy['occupation_skill_3_count'].mode()[0])\n",
    "    data_copy['occupation_skill_4_count']=data_copy['occupation_skill_4_count'].fillna(data_copy['occupation_skill_4_count'].mode()[0])\n",
    "    data_copy['occupation_skill_5_count']=data_copy['occupation_skill_5_count'].fillna(data_copy['occupation_skill_5_count'].mode()[0])\n",
    "    data_copy['occupation_skill_6_count']=data_copy['occupation_skill_6_count'].fillna(data_copy['occupation_skill_6_count'].mode()[0])\n",
    "    data_copy['occupation_skill_7_count']=data_copy['occupation_skill_7_count'].fillna(data_copy['occupation_skill_7_count'].mode()[0])\n",
    "    data_copy['occupation_skill_8_count']=data_copy['occupation_skill_8_count'].fillna(data_copy['occupation_skill_8_count'].mode()[0])\n",
    "    data_copy['occupation_skill_9_count']=data_copy['occupation_skill_9_count'].fillna(data_copy['occupation_skill_9_count'].mode()[0])\n",
    "\n",
    "    data_copy['candidate_attribute_1']=data_copy['candidate_attribute_1'].fillna(data_copy['candidate_attribute_1'].mode()[0])\n",
    "    data_copy['candidate_attribute_2']=data_copy['candidate_attribute_2'].fillna(round(data_copy['candidate_attribute_2'].mean(),4))\n",
    "    data_copy['candidate_attribute_3']=data_copy['candidate_attribute_3'].fillna(data_copy['candidate_attribute_3'].mode()[0])\n",
    "    data_copy['candidate_attribute_4']=data_copy['candidate_attribute_4'].fillna(data_copy['candidate_attribute_4'].mode()[0])\n",
    "    data_copy['candidate_attribute_5']=data_copy['candidate_attribute_5'].fillna(data_copy['candidate_attribute_5'].mode()[0])\n",
    "    data_copy['candidate_attribute_6']=data_copy['candidate_attribute_6'].fillna(data_copy['candidate_attribute_6'].mode()[0])\n",
    "    data_copy['candidate_attribute_7']=data_copy['candidate_attribute_7'].fillna(data_copy['candidate_attribute_7'].mode()[0])\n",
    "    data_copy['candidate_attribute_8']=data_copy['candidate_attribute_8'].fillna(round(data_copy['candidate_attribute_8'].mean()))\n",
    "\n",
    "    data_copy['candidate_interest_1']=data_copy['candidate_interest_1'].fillna(data_copy['candidate_interest_1'].mode()[0])\n",
    "    data_copy['candidate_interest_3']=data_copy['candidate_interest_3'].fillna(data_copy['candidate_interest_3'].mode()[0])\n",
    "    data_copy['candidate_interest_4']=data_copy['candidate_interest_4'].fillna(data_copy['candidate_interest_4'].mode()[0])\n",
    "    data_copy['candidate_interest_5']=data_copy['candidate_interest_5'].fillna(data_copy['candidate_interest_5'].mode()[0])\n",
    "    data_copy['candidate_interest_6']=data_copy['candidate_interest_6'].fillna(data_copy['candidate_interest_6'].mode()[0])\n",
    "    data_copy['candidate_interest_7']=data_copy['candidate_interest_7'].fillna(data_copy['candidate_interest_7'].mode()[0])\n",
    "    data_copy['candidate_interest_8']=data_copy['candidate_interest_8'].fillna(data_copy['candidate_interest_8'].mode()[0])\n",
    "\n",
    "    data_copy['number_years_feature_1']=data_copy['number_years_feature_1'].fillna(round(data_copy['number_years_feature_1'].mean(),6))\n",
    "    data_copy['number_years_feature_2']=data_copy['number_years_feature_2'].fillna(round(data_copy['number_years_feature_2'].mean(),6))\n",
    "    data_copy['number_years_feature_3']=data_copy['number_years_feature_3'].fillna(round(data_copy['number_years_feature_3'].mean(),6))\n",
    "    data_copy['number_years_feature_4']=data_copy['number_years_feature_4'].fillna(round(data_copy['number_years_feature_4'].mean(),6))\n",
    "    data_copy['number_years_feature_5']=data_copy['number_years_feature_5'].fillna(round(data_copy['number_years_feature_5'].mean(),6))\n",
    "\n",
    "    data_copy['candidate_skill_1_count']=data_copy['candidate_skill_1_count'].fillna(round(data_copy['candidate_skill_1_count'].mean()))\n",
    "    data_copy['candidate_skill_2_count']=data_copy['candidate_skill_2_count'].fillna(round(data_copy['candidate_skill_2_count'].mean()))\n",
    "    data_copy['candidate_skill_3_count']=data_copy['candidate_skill_3_count'].fillna(round(data_copy['candidate_skill_3_count'].mean()))\n",
    "    data_copy['candidate_skill_4_count']=data_copy['candidate_skill_4_count'].fillna(round(data_copy['candidate_skill_4_count'].mean()))\n",
    "    data_copy['candidate_skill_5_count']=data_copy['candidate_skill_5_count'].fillna(round(data_copy['candidate_skill_5_count'].mean()))\n",
    "    data_copy['candidate_skill_6_count']=data_copy['candidate_skill_6_count'].fillna(round(data_copy['candidate_skill_6_count'].mean()))\n",
    "    data_copy['candidate_skill_7_count']=data_copy['candidate_skill_7_count'].fillna(round(data_copy['candidate_skill_7_count'].mean()))\n",
    "    data_copy['candidate_skill_8_count']=data_copy['candidate_skill_8_count'].fillna(round(data_copy['candidate_skill_8_count'].mean()))\n",
    "    data_copy['candidate_skill_9_count']=data_copy['candidate_skill_9_count'].fillna(round(data_copy['candidate_skill_9_count'].mean()))\n",
    "\n",
    "    data_copy['candidate_relative_test_1']=data_copy['candidate_relative_test_1'].fillna(round(data_copy['candidate_relative_test_1'].mean(),6))\n",
    "    data_copy['candidate_relative_test_2']=data_copy['candidate_relative_test_2'].fillna(round(data_copy['candidate_relative_test_2'].mean(),6))\n",
    "\n",
    "    x = data_copy.drop(['application_status', 'candidate_id','occupation_id','company_id','ethnicity','gender','age',\\\n",
    "                        'application_attribute_1', 'candidate_interest_2',\\\n",
    "                        'candidate_demographic_variable_1', 'candidate_demographic_variable_2',\\\n",
    "                        'candidate_demographic_variable_3', 'candidate_demographic_variable_4',\\\n",
    "                        'candidate_demographic_variable_5', 'candidate_demographic_variable_6',\\\n",
    "                        'candidate_demographic_variable_7', 'candidate_demographic_variable_8',\\\n",
    "                        'candidate_demographic_variable_9', 'candidate_demographic_variable_10'],axis=1)\n",
    "\n",
    "    y = data_copy['application_status']\n",
    "\n",
    "    #Splitting data 90 and 10% to test systematicity\n",
    "\n",
    "    x_train_systemicity, x_test_systemicity, y_train_systemicity, y_test_systemicity = train_test_split(x, y, test_size = 0.1, random_state = 28)\n",
    "\n",
    "    return x_train_systemicity, x_test_systemicity, y_train_systemicity, y_test_systemicity\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae35ca1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Phase1 (Trainning the ensemble model): Training and building an ensemble model that contains a set of models (with similar accuracy). Storing the set of models to use at prediction time.\n",
    "\n",
    "def train_ensembleModel(x_train_systemicity, x_test_systemicity, y_train_systemicity, y_test_systemicity, ensenbleModel):\n",
    "\n",
    "    all_models=[]\n",
    "    TP_set_list=[]\n",
    "    FP_set_list=[]\n",
    "    FN_set_list=[]\n",
    "    TN_set_list=[]\n",
    "    all_TP=set()\n",
    "    all_TP_len=[]\n",
    "    all_FP=set()\n",
    "    all_FP_len=[]\n",
    "    all_FN=set()\n",
    "    all_FN_len=[]\n",
    "    all_TN=set()\n",
    "    all_TN_len=[]\n",
    "    last_tp_len=0\n",
    "    last_fp_len=0\n",
    "    last_fn_len=0\n",
    "    last_tn_len=0\n",
    "    total_hired_test_systematicity_set=set(y_test_systemicity.loc[y_test_systemicity=='hired'].index)\n",
    "\n",
    "    all_TP_len_diff=[]\n",
    "    last_ten_TP_len_update=[0]*10\n",
    "\n",
    "    i_tp=0\n",
    "\n",
    "    flg_strt_running_avg=False\n",
    "\n",
    "    #500 times randomly split the train-test data into 80% for training and 20% for testing \n",
    "\n",
    "    for i in range(500):\n",
    "        #Train model using random forest classifier on each split\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_train_systemicity, y_train_systemicity, test_size = 0.2, random_state = i)\n",
    "        randomForest = RandomForestClassifier()\n",
    "        randomForest.set_params(random_state=42,class_weight='balanced',criterion='entropy',min_samples_leaf= 10, min_samples_split= 28, n_estimators= 200)\n",
    "        randomForest.fit(x_train, y_train) \n",
    "        y_pred=randomForest.predict(x_test)\n",
    "        #print(\"Confusion matrx OF THE MODEL: \", metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "        #Compute recall and f1-score of 'hired' class\n",
    "        TP_hired=0\n",
    "        FN_hired=0\n",
    "        FP_hired=0\n",
    "        TN_hired=0\n",
    "        i=0\n",
    "\n",
    "        for items in y_test.iteritems():\n",
    "            if items[1]==\"hired\":\n",
    "                if y_pred[i]==\"hired\":\n",
    "                    TP_hired=TP_hired+1\n",
    "                else:\n",
    "                    FN_hired=FN_hired+1\n",
    "            else:\n",
    "                if y_pred[i]==\"hired\":\n",
    "                    FP_hired=FP_hired+1\n",
    "                else:\n",
    "                    TN_hired=TN_hired+1\n",
    "\n",
    "\n",
    "            i=i+1            \n",
    "\n",
    " \n",
    "        recall_hired=TP_hired/(TP_hired+FN_hired)\n",
    "        precision_hired=TP_hired/(TP_hired+FP_hired)\n",
    "        f1_hired=(2 * precision_hired * recall_hired) / (precision_hired + recall_hired)\n",
    "\n",
    "\n",
    "    #Select the models that have a recall (of hired candidates) and f1-score(of hired candidates) above a certain threshold\n",
    "\n",
    "        if recall_hired>=0.63 and f1_hired>=0.58:  #Ensuring accuracy\n",
    "            y_pred_systemicity=randomForest.predict(x_test_systemicity)\n",
    "            TP_hired_systemicity=0\n",
    "            FN_hired_systemicity=0\n",
    "            FP_hired_systemicity=0\n",
    "            TN_hired_systemicity=0\n",
    "            i_systemicity=0\n",
    "            TP_set_systemicity=set()\n",
    "            FN_set_systemicity=set()\n",
    "            FP_set_systemicity=set()\n",
    "            TN_set_systemicity=set()\n",
    "            for items_systemicity in y_test_systemicity.iteritems():\n",
    "                if items_systemicity[1]==\"hired\":\n",
    "                    if y_pred_systemicity[i_systemicity]==\"hired\":\n",
    "                        TP_hired_systemicity=TP_hired_systemicity+1\n",
    "                        TP_set_systemicity.add(items_systemicity[0])\n",
    "                    else:\n",
    "                        FN_hired_systemicity=FN_hired_systemicity+1\n",
    "                        FN_set_systemicity.add(items_systemicity[0])\n",
    "                else:\n",
    "                    if y_pred_systemicity[i_systemicity]==\"hired\":\n",
    "                        FP_hired_systemicity=FP_hired_systemicity+1\n",
    "                        FP_set_systemicity.add(items_systemicity[0])\n",
    "                    else:\n",
    "                        TN_hired_systemicity=TN_hired_systemicity+1\n",
    "                        TN_set_systemicity.add(items_systemicity[0])\n",
    "\n",
    "                i_systemicity=i_systemicity+1 \n",
    "\n",
    "\n",
    "            if i_tp>=10:\n",
    "                i_tp=0\n",
    "                flg_strt_running_avg=True\n",
    "\n",
    "\n",
    "            if len(TP_set_list)==0:\n",
    "                all_models.append(randomForest)\n",
    "                TP_set_list.append(TP_set_systemicity)\n",
    "                all_TP=all_TP.union(TP_set_systemicity)\n",
    "\n",
    "                FP_set_list.append(FP_set_systemicity)\n",
    "                all_FP=all_FP.union(FP_set_systemicity)\n",
    "                FN_set_list.append(FN_set_systemicity)\n",
    "                all_FN=all_FN.union(FN_set_systemicity)\n",
    "                TN_set_list.append(TN_set_systemicity)\n",
    "                all_TN=all_TN.union(TN_set_systemicity)\n",
    "                all_TP_len.append(len(all_TP))\n",
    "                all_FP_len.append(len(all_FP))\n",
    "                all_FN_len.append(len(all_FN))\n",
    "                all_TN_len.append(len(all_TN))\n",
    "                last_tp_len=len(all_TP)\n",
    "                #computing compute the running average of the change in TP set size of the last 10 accepted models\n",
    "                all_TP_len_diff.append(len(total_hired_test_systematicity_set-all_TP))\n",
    "\n",
    "            else:\n",
    "                flg=True\n",
    "                for itm in TP_set_list:\n",
    "                    if len(TP_set_systemicity-itm)==0:\n",
    "                        flg=False\n",
    "                        break\n",
    "                if flg==True:\n",
    "                    all_models.append(randomForest)\n",
    "                    TP_set_list.append(TP_set_systemicity)\n",
    "                    all_TP=all_TP.union(TP_set_systemicity)\n",
    "                    FP_set_list.append(FP_set_systemicity)\n",
    "                    all_FP=all_FP.union(FP_set_systemicity)\n",
    "                    FN_set_list.append(FN_set_systemicity)\n",
    "                    all_FN=all_FN.union(FN_set_systemicity)\n",
    "                    TN_set_list.append(TN_set_systemicity)\n",
    "                    all_TN=all_TN.union(TN_set_systemicity)\n",
    "                    all_TP_len.append(len(all_TP))\n",
    "                    all_FP_len.append(len(all_FP))\n",
    "                    all_FN_len.append(len(all_FN))\n",
    "                    all_TN_len.append(len(all_TN))\n",
    "                    last_ten_TP_len_update[i_tp]=abs(last_tp_len-len(all_TP))\n",
    "                    last_tp_len=len(all_TP)\n",
    "                    i_tp=i_tp+1\n",
    "                    all_TP_len_diff.append(len(total_hired_test_systematicity_set-all_TP))\n",
    "\n",
    "\n",
    "        #Stop if the union of all ensemble model’s true positives covers all of the possible true positives in the systematicity dataset\n",
    "        if len(total_hired_test_systematicity_set-all_TP)==0:\n",
    "            break\n",
    "\n",
    "        #We stop when the running average is 0 \n",
    "        if flg_strt_running_avg==True:\n",
    "            running_avg_diff=sum(last_ten_TP_len_update) / len(last_ten_TP_len_update)\n",
    "            if running_avg_diff==0:  #if last 10 model has not added atleast one extra new TP\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "    #Save model to disk\n",
    "    mdl_set_len=len(all_models)\n",
    "    for k in range(mdl_set_len):\n",
    "        s1=\"Model\"\n",
    "        s2=\".joblib\"\n",
    "        filename = r'{}/{}{}{}'.format(ensenbleModel, s1, k, s2)\n",
    "    #     print(filename)\n",
    "        joblib.dump(all_models[k], filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b9162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Phase2: (Predicting) Randomly select one model from the ensemble model set at prediction time\n",
    "def predict(new_candidate,Ensembles1):\n",
    "    dir_path = r'{}/'.format(Ensembles1)\n",
    "    count = len(fnmatch.filter(os.listdir(dir_path), '*.joblib'))\n",
    "    arr_pred= np.empty(len(new_candidate), dtype=object)\n",
    "    i=0\n",
    "    for rw in range(len(new_candidate)):\n",
    "        candidate=new_candidate.iloc[[rw]]\n",
    "        mdl_idx=random.randint(0, (count-1))\n",
    "        s1=\"Model\"\n",
    "        s2=\".joblib\"\n",
    "        selectedfile = r'{}/{}{}{}'.format(Ensembles1,s1, mdl_idx, s2)\n",
    "        selected_model = joblib.load(selectedfile)\n",
    "        arr_pred[i]=(selected_model.predict(candidate)[0])\n",
    "        i=i+1\n",
    "\n",
    "    return(arr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ce7c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation metrics\n",
    "\n",
    "\n",
    "def evaluate(test_data,Ensembles):\n",
    "    test_data_copy=test_data.loc[test_data['Application_status']==\"hired\"]\n",
    "    x_test_eval=test_data_copy.drop(['Application_status'],axis=1)\n",
    "    all_hired_ln=len(x_test_eval)\n",
    "    freq_arr = np.zeros(all_hired_ln)\n",
    "    dir_path = r'{}/'.format(Ensembles)\n",
    "    count = len(fnmatch.filter(os.listdir(dir_path), '*.joblib'))\n",
    "    model_indx=0\n",
    "    for rw in range(count):\n",
    "        s1=\"Model\"\n",
    "        s2=\".joblib\"\n",
    "        selectedfile = r'{}/{}{}{}'.format(Ensembles,s1, model_indx, s2)\n",
    "        selected_model = joblib.load(selectedfile)\n",
    "        y_pred_eval=selected_model.predict(x_test_eval)\n",
    "        model_indx=model_indx+1\n",
    "    \n",
    "        indx=0\n",
    "        for i in y_pred_eval:\n",
    "            if i==\"hired\":\n",
    "                freq_arr[indx]=freq_arr[indx]+1\n",
    "            indx=indx+1    \n",
    "\n",
    "    \n",
    "############Distribution chart\n",
    "    df = pd.DataFrame({ 'TP_Frequency': freq_arr })\n",
    "    df = df.sort_values(by=\"TP_Frequency\", ascending=False)\n",
    "    df['Candidates'] = range(all_hired_ln)\n",
    "    df['Count']=count\n",
    "    df['TP_Frequency%']=(df['TP_Frequency']*100)/df['Count']\n",
    "    fig = plt.scatter(df, x='Candidates', y='TP_Frequency%', title=\"TP frequency distribution\" ,labels={\n",
    "                     \"TP_Frequency%\": \"Chance of being selected\"\n",
    "                 } )\n",
    "    fig.show()\n",
    "    fig.write_html(\"TP_Frequency.html\")\n",
    "    \n",
    "#######Metric1: Coverage (Indicates fairness of the random classifier ensamble)\n",
    "    all_TP=len(np.nonzero(freq_arr)[0])\n",
    "    coverage=all_TP/all_hired_ln\n",
    "\n",
    "    \n",
    "    \n",
    "#######Metric2: Arbitrariness (Indicates the arbitrariness of the model and the distribution of true positive)\n",
    "    TP_freq_mean=np.mean(freq_arr)\n",
    "    print(TP_freq_mean)\n",
    "    sum=0\n",
    "    for j in freq_arr:\n",
    "        sum=sum+abs(j-TP_freq_mean)\n",
    "\n",
    "    arbitrariness=(sum/all_hired_ln)\n",
    "    \n",
    "    \n",
    "    return coverage, arbitrariness\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "def30c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the main code block\n",
    "\n",
    "\n",
    "data=pd.read_csv(\"data.csv\")\n",
    "data_copy=data.copy()\n",
    "# Spliting the dataset randomly into 2 parts: one with 90% and one with 10% of the data\n",
    "# The 90% is used to create models and the 10% is used to address systematicity\n",
    "# Let’s call the 10% “systematicity data” and the 90% “train-test data”\n",
    "x_train_systemicity, x_test_systemicity, y_train_systemicity, y_test_systemicity=clean_split_data(data_copy)\n",
    "\n",
    "# Training models for Predict() function\n",
    "ensembleModel=\"RandomClassifierEnsembles\"\n",
    "train_ensembleModel(x_train_systemicity, x_test_systemicity, y_train_systemicity, y_test_systemicity, ensembleModel)\n",
    "\n",
    "# Randomly select one model from the ensemble model set at prediction time.\n",
    "new_candidate_df=x_test_systemicity.head(5)\n",
    "lst=predict(new_candidate_df,ensembleModel)\n",
    "\n",
    "#Evaluation Metrics\n",
    "ensembleModel=\"RandomClassifierEnsembles\"\n",
    "test_data=x_test_systemicity.copy()\n",
    "test_data['Application_status']=y_test_systemicity\n",
    "coverage, arbitrariness=evaluate(test_data,ensembleModel)\n",
    "print(\"Coverage of \",ensembleModel,\" is :\", coverage)\n",
    "print(\"Arbitrariness of \",ensembleModel,\" is :\", arbitrariness)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
